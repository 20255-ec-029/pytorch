{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units, known as tokens. Tokens can be words, phrases, symbols, or other meaningful elements like punctuation marks. Here’s why tokenization is crucial in NLP:\n",
    "\n",
    "1. Simplifies Text Processing\n",
    "Text data in its raw form is unstructured and complex. Tokenization breaks this text into manageable pieces, making it easier for algorithms to process.\n",
    "2. Forms the Basis for Further Processing\n",
    "Most NLP techniques, such as stemming, lemmatization, and part-of-speech tagging, require input in the form of individual words or subwords. Tokenization is the first step to facilitate these operations.\n",
    "3. Improves Model Input\n",
    "Machine learning models, including neural networks and transformers, typically work with numerical data. Tokenization helps map words or phrases into a structured format (like word embeddings) that models can understand.\n",
    "4. Enables Context-Aware Models\n",
    "In advanced models like BERT and GPT, tokenization often includes breaking down text into subword units, allowing the model to understand the meaning of complex words and handle unknown or rare words more effectively.\n",
    "Types of Tokenization:\n",
    "Word Tokenization: Splits text into individual words. For example, \"I love NLP\" becomes [\"I\", \"love\", \"NLP\"].\n",
    "Subword Tokenization: Breaks words into smaller units. For example, \"unhappiness\" could be split into [\"un\", \"happiness\"].\n",
    "Sentence Tokenization: Divides text into sentences, useful for processing longer texts or documents.\n",
    "In summary, tokenization is a fundamental step in NLP that allows further processing and analysis of text, facilitating a range of language understanding tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "NLTK is a Python library used in natural language processing (NLP) for tasks such as tokenization and text processing. The code example shows how you can tokenize text using the NLTK word-based tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt',download_dir = '/home/codespace/nltk_data/')\n",
    "\n",
    "# # nltk.download('punkt', download_dir='/tmp/lib/nltk_data')\n",
    "# nltk.data.path.append('/home/codespace/nltk_data/')\n",
    "\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\"\n",
    "# token = word_tokenize(text)\n",
    "# print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy \n",
    "spaCy is an open-source library used in NLP. It provides tools for tasks such as tokenization and word embeddings. The code example shows how you can tokenize text using spaCy word-based tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.', 'I', 'could', \"n't\", 'see', 'it', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertTokenizer: \n",
    "BertTokenizer is a subword-based tokenizer that uses the WordPiece algorithm. The code example shows how you can tokenize text using BertTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNetTokenizer :\n",
    "XLNetTokenizer tokenizes text using Unigram and SentencePiece algorithms. The code example shows how you can tokenize text using XLNetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁IBM', '▁taught', '▁me', '▁token', 'ization', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "0.15.2+cpu\n",
      "0.6.1\n"
     ]
    }
   ],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "print(torch.__version__)       \n",
    "print(torchtext.__version__)   \n",
    "print(torchdata.__version__)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchtext\n",
    "The torchtext library is part of the PyTorch ecosystem and provides the tools and functionalities required for NLP. The code example shows how you can use `torchtext to generate tokens and convert them to indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vs': 21, 'to': 19, 'of': 15, 'introduction': 14, 'recognition': 16, 'for': 13, 'nlp': 1, 'entity': 4, 'pytorch': 2, 'translation': 8, 'techniques': 17, 'machine': 5, 'named': 6, ',': 10, 'text': 18, 'sentiment': 7, '<unk>': 0, 'with': 9, 'basics': 11, 'using': 20, 'analysis': 3, 'classification': 12}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vs': 21,\n",
       " 'to': 19,\n",
       " 'of': 15,\n",
       " 'introduction': 14,\n",
       " 'recognition': 16,\n",
       " 'for': 13,\n",
       " 'nlp': 1,\n",
       " 'entity': 4,\n",
       " 'pytorch': 2,\n",
       " 'translation': 8,\n",
       " 'techniques': 17,\n",
       " 'machine': 5,\n",
       " 'named': 6,\n",
       " ',': 10,\n",
       " 'text': 18,\n",
       " 'sentiment': 7,\n",
       " '<unk>': 0,\n",
       " 'with': 9,\n",
       " 'basics': 11,\n",
       " 'using': 20,\n",
       " 'analysis': 3,\n",
       " 'classification': 12}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defines a dataset\n",
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\"NLP Named Entity,Sentiment Analysis, Machine Translation\"),\n",
    "    (1,\"Machine Translation with NLP\"),\n",
    "    (1,\"Named Entity vs Sentiment Analysis NLP\")]\n",
    "# Applies the tokenizer to the text to get the tokens as a list\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# Takes a data iterator as input, processes text from the iterator, \n",
    "# and yields the tokenized output individually\n",
    "def yield_tokens(data_iter):\n",
    "    for _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "# Creates an iterator\n",
    "my_iterator = yield_tokens(dataset)\n",
    "# Fetches the next set of tokens from the data set\n",
    "next(my_iterator)\n",
    "# Converts tokens to indices and sets <unk> as the \n",
    "# default word if a word is not found in the vocabulary\n",
    "vocab = []\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "print(vocab.get_stoi())\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# Gives a dictionary that maps words to their corresponding numerical indices\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV Tokens: set()\n"
     ]
    }
   ],
   "source": [
    "# Check for OOV tokens: Out of Vocaab.\n",
    "oov_tokens = set()  # Use a set to store unique OOV tokens\n",
    "for _, text in dataset:\n",
    "    tokens = tokenizer(text)\n",
    "    for token in tokens:\n",
    "        # If the token maps to '<unk>', it is an OOV token\n",
    "        if vocab[token] == vocab[\"<unk>\"]:\n",
    "            oov_tokens.add(token)\n",
    "\n",
    "print(f\"OOV Tokens: {oov_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab:\n",
    "The vocab object is part of the PyTorch torchtext library. It maps tokens to indices. The code example shows how you can apply the vocab object to tokens directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
      "Token Indices: [11, 15, 2]\n"
     ]
    }
   ],
   "source": [
    "# Takes an iterator as input and extracts the next tokenized sentence. \n",
    "# Creates a list of token indices using the vocab dictionary for each token.\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices\n",
    "# Returns the tokenized sentences and the corresponding token indices. \n",
    "# Repeats the process.\n",
    "tokenized_sentence, token_indices = \\\n",
    "get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "# Prints the tokenized sentence and its corresponding token indices.\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens in PyTorch: `<eos>` and  `<bos>`\n",
    "Special tokens are tokens introduced to input sequences to convey specific information or serve a particular purpose during training. The code example shows the use of <bos> and <eos> during tokenization. The <bos> token denotes the beginning of the input sequence, and the <eos> token denotes the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', 'I', 'am', 'vamshi', 'dhar', '<eos>'], ['<bos>', 'a', 'data', 'scientist', '<eos>']]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Appends <bos> at the beginning and <eos> at the end of the tokenized sentences \n",
    "# using a loop that iterates over the sentences in the input data\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = []\n",
    "lines = ['I am vamshi dhar', 'a data scientist']\n",
    "max_length = 0\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "print(tokens)\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens in PyTorch: `<pad>`\t\n",
    "The code example shows the use of <pad> token to ensure all sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', 'I', 'am', 'vamshi', 'dhar', '<eos>'], ['<bos>', 'a', 'data', 'scientist', '<eos>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "# # Pads the tokenized lines\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class in PyTorch:\t\n",
    "The Dataset class enables accessing and retrieving individual samples from a data set. The code example shows how you can create a custom data set and access samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports the Dataset class and defines a list of sentences\n",
    "from torch.utils.data import Dataset\n",
    "sentences = [\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\"]\n",
    "# Downloads and reads data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "    # Returns the data length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    # Returns one item on the index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "# Creates a dataset object\n",
    "dataset=CustomDataset(sentences)\n",
    "# Accesses samples like in a list\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 1\n",
      "Sentence 0: If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of sentences in the dataset\n",
    "length_of_dataset = len(dataset)\n",
    "print(f\"Length of the dataset: {length_of_dataset}\")\n",
    "\n",
    "# Access each sentence in the dataset using its index\n",
    "for idx in range(len(dataset)):\n",
    "    print(f\"Sentence {idx}: {sentences[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader class in PyTorch:\t\n",
    "A DataLoader class enables efficient loading and iteration over data sets for training deep learning models. The code example shows how you can use the DataLoader class to generate batches of sentences for further processing, such as training a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an instance of the custom data set\n",
    "from torch.utils.data import DataLoader\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "# Specifies a batch size\n",
    "batch_size = 2\n",
    "# Creates a data loader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using for loop : [\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\"]\n",
      "using iter: [\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\"]\n"
     ]
    }
   ],
   "source": [
    "#Using each batch\n",
    "\n",
    "### using for loop:\n",
    "# Prints the sentences in each batch\n",
    "for batch in dataloader:\n",
    "    print(f\"using for loop : {batch}\")\n",
    "    \n",
    "####   or\n",
    "\n",
    "### using for iter:\n",
    "# Creates an iterator object\n",
    "data_iter = iter(dataloader)\n",
    "# Calls the next function to return new batches of samples\n",
    "print(f\"using iter: {next(data_iter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom collate function in PyTorch:\t\n",
    "The custom collate function is a user-defined function that defines how individual samples are collated or batched together. You can utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor. The code example shows how you can use a custom collate function in a data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0,  0,  0, 19,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torch\n",
    "# Defines a custom collate function\n",
    "def collate_fn(batch):\n",
    "    tensor_batch = []\n",
    "# Tokenizes each sample in the batch\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "# Maps tokens to numbers using the vocab\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "# Pads the sequences within the batch to have equal lengths\n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
    "    return padded_batch\n",
    "# Creates a data loader using the collate function and the custom dataset\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "print([x for x in dataloader])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
