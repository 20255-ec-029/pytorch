{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I am Vamshidhar H K, working as a data scientist at Flytxt\",\n",
    "    \"I am a native of Andhra pradesh\",\n",
    "    \"I am passionate about data analytics in various domains Teleco, banking and health care\",\n",
    "    \"I am currnelty focussed on leanring NLP from Coursera\",\n",
    "    \"I admire proff. Gilbert strang as he teaches Maths in a most simplest way.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "### Tokenization:\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sent in sentences:\n",
    "    tokenized_sentences.append(tokenizer(sent)) # tokenizer requires each sample and not the list of samples.\n",
    "print(f'full data after first tokenization:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences:\n",
    "    print(i)\n",
    "\n",
    "### Adding special tokens:\n",
    "tokenized_sentences_new = []\n",
    "max_length = 0\n",
    "for tokenized_sent in tokenized_sentences:\n",
    "    tokenized_sentence_new = ['<bos>'] + tokenized_sent + ['<eos>']\n",
    "    tokenized_sentences_new.append(tokenized_sentence_new)\n",
    "    max_length = max(max_length, len(tokenized_sentence_new))\n",
    "print(f'max length : {max_length}')\n",
    "print(f'full data after adding <bos> & <eos>:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)\n",
    "### Adding special tokens:\n",
    "tokenized_sentences_new = []\n",
    "max_length = 0\n",
    "for tokenized_sent in tokenized_sentences:\n",
    "    tokenized_sentence_new = ['<bos>'] + tokenized_sent + ['<eos>']\n",
    "    tokenized_sentences_new.append(tokenized_sentence_new)\n",
    "    max_length = max(max_length, len(tokenized_sentence_new))\n",
    "print(f'max length : {max_length}')\n",
    "print(f'full data after adding <bos> & <eos>:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)\n",
    "\n",
    "### It is observed that all have different lengths and max is 18, so lets do padding\n",
    "###  to maintain same length for each sentence.\n",
    "for i in range(len(tokenized_sentences_new)):\n",
    "    tokenized_sentences_new[i] = tokenized_sentences_new[i] + ['<pad>'] * (max_length - len(tokenized_sentences_new[i]))\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)\n",
    "\n",
    "### \"Vocabulary Building\" or \"Vocabulary Construction\":\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<unk>\"]) \n",
    "vocab.set_default_index(vocab[\"<unk>\"]) \n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "# this will give indexs : {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "# It is required to provide unique index to each spl token for the model to differentiate each token separately,\n",
    "# or else it will be problematic when model predicts 0 index, and it is difficult to decide which spl token it is refering to. \n",
    "#: It is a common convention to assign the padding token the index 0, so keep <pad> at first,\n",
    "#: and for rest of the tokens, any number is provided. \n",
    "\n",
    "### Indexing\n",
    "indexed_tokens = []\n",
    "for tokenized_sent in tokenized_sentences_new:\n",
    "    indexed_tokens.append([vocab[x] for x in tokenized_sent])\n",
    "print(f'indexed last sample: {indexed_tokens[5]}')\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "for i in indexed_tokens:\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = [\n",
    "    \"I am Vamshidhar H K, working as a data scientist at Flytxt\",\n",
    "    \"I am a native of Andhra pradesh\",\n",
    "    \"I am passionate about data analytics in various domains Teleco, banking and health care\",\n",
    "    \"I am currnelty focussed on leanring NLP from Coursera\",\n",
    "    \"I admire proff. Gilbert strang as he teaches Maths in a most simplest way.\",\n",
    "    \"You are awesome!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manully processing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full data after first tokenization:---------------------------------------------------------------\n",
      "['i', 'am', 'vamshidhar', 'h', 'k', ',', 'working', 'as', 'a', 'data', 'scientist', 'at', 'flytxt']\n",
      "['i', 'am', 'a', 'native', 'of', 'andhra', 'pradesh']\n",
      "['i', 'am', 'passionate', 'about', 'data', 'analytics', 'in', 'various', 'domains', 'teleco', ',', 'banking', 'and', 'health', 'care']\n",
      "['i', 'am', 'currnelty', 'focussed', 'on', 'leanring', 'nlp', 'from', 'coursera']\n",
      "['i', 'admire', 'proff', '.', 'gilbert', 'strang', 'as', 'he', 'teaches', 'maths', 'in', 'a', 'most', 'simplest', 'way', '.']\n",
      "['you', 'are', 'awesome', '!']\n"
     ]
    }
   ],
   "source": [
    "### Tokenization:\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sent in sentences:\n",
    "    tokenized_sentences.append(tokenizer(sent)) # tokenizer requires each sample and not the list of samples.\n",
    "print(f'full data after first tokenization:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length : 18\n",
      "full data after adding <bos> & <eos>:---------------------------------------------------------------\n",
      "['<bos>', 'i', 'am', 'vamshidhar', 'h', 'k', ',', 'working', 'as', 'a', 'data', 'scientist', 'at', 'flytxt', '<eos>']\n",
      "['<bos>', 'i', 'am', 'a', 'native', 'of', 'andhra', 'pradesh', '<eos>']\n",
      "['<bos>', 'i', 'am', 'passionate', 'about', 'data', 'analytics', 'in', 'various', 'domains', 'teleco', ',', 'banking', 'and', 'health', 'care', '<eos>']\n",
      "['<bos>', 'i', 'am', 'currnelty', 'focussed', 'on', 'leanring', 'nlp', 'from', 'coursera', '<eos>']\n",
      "['<bos>', 'i', 'admire', 'proff', '.', 'gilbert', 'strang', 'as', 'he', 'teaches', 'maths', 'in', 'a', 'most', 'simplest', 'way', '.', '<eos>']\n",
      "['<bos>', 'you', 'are', 'awesome', '!', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "### Adding special tokens:\n",
    "tokenized_sentences_new = []\n",
    "max_length = 0\n",
    "for tokenized_sent in tokenized_sentences:\n",
    "    tokenized_sentence_new = ['<bos>'] + tokenized_sent + ['<eos>']\n",
    "    tokenized_sentences_new.append(tokenized_sentence_new)\n",
    "    max_length = max(max_length, len(tokenized_sentence_new))\n",
    "print(f'max length : {max_length}')\n",
    "print(f'full data after adding <bos> & <eos>:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full data after padding :---------------------------------------------------------------\n",
      "['<bos>', 'i', 'am', 'vamshidhar', 'h', 'k', ',', 'working', 'as', 'a', 'data', 'scientist', 'at', 'flytxt', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', 'am', 'a', 'native', 'of', 'andhra', 'pradesh', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', 'am', 'passionate', 'about', 'data', 'analytics', 'in', 'various', 'domains', 'teleco', ',', 'banking', 'and', 'health', 'care', '<eos>', '<pad>']\n",
      "['<bos>', 'i', 'am', 'currnelty', 'focussed', 'on', 'leanring', 'nlp', 'from', 'coursera', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', 'admire', 'proff', '.', 'gilbert', 'strang', 'as', 'he', 'teaches', 'maths', 'in', 'a', 'most', 'simplest', 'way', '.', '<eos>']\n",
      "['<bos>', 'you', 'are', 'awesome', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "### It is observed that all have different lengths and max is 18, so lets do padding\n",
    "###  to maintain same length for each sentence.\n",
    "for i in range(len(tokenized_sentences_new)):\n",
    "    tokenized_sentences_new[i] = tokenized_sentences_new[i] + ['<pad>'] * (max_length - len(tokenized_sentences_new[i]))\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed last sample: [1, 53, 18, 20, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "full data after padding :---------------------------------------------------------------\n",
      "tensor([ 1,  4,  5, 49, 30, 33,  7, 52,  9,  6, 10, 44, 19, 26,  2,  0,  0,  0])\n",
      "tensor([ 1,  4,  5,  6, 37, 39, 17, 42,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 1,  4,  5, 41, 13, 10, 15, 11, 50, 25, 48,  7, 21, 16, 32, 22,  2,  0])\n",
      "tensor([ 1,  4,  5, 24, 27, 40, 34, 38, 28, 23,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 1,  4, 14, 43,  8, 29, 46,  9, 31, 47, 35, 11,  6, 36, 45, 51,  8,  2])\n",
      "tensor([ 1, 53, 18, 20, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "### \"Vocabulary Building\" or \"Vocabulary Construction\":\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<unk>\"]) \n",
    "vocab.set_default_index(vocab[\"<unk>\"]) \n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "# this will give indexs : {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "# It is required to provide unique index to each spl token for the model to differentiate each token separately,\n",
    "# or else it will be problematic when model predicts 0 index, and it is difficult to decide which spl token it is refering to. \n",
    "#: It is a common convention to assign the padding token the index 0, so keep <pad> at first,\n",
    "#: and for rest of the tokens, any number is provided. \n",
    "\n",
    "# Assign specific indices if required\n",
    "\"\"\"\n",
    "vocab.get_stoi()[\"<pad>\"] = 0\n",
    "vocab.get_stoi()[\"<bos>\"] = 1\n",
    "vocab.get_stoi()[\"<eos>\"] = 2\n",
    "vocab.get_stoi()[\"<unk>\"] = 3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### Indexing\n",
    "indexed_tokens = []\n",
    "for tokenized_sent in tokenized_sentences_new:\n",
    "    indexed_tokens.append([vocab[x] for x in tokenized_sent])\n",
    "print(f'indexed last sample: {indexed_tokens[5]}')\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "import torch\n",
    "for i in indexed_tokens:\n",
    "    print(torch.tensor(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CustomDataset class and collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset Length: 6\n",
      "Sample Items:\n",
      "Item 1: I am Vamshidhar H K, working as a data scientist at Flytxt\n",
      "Item 2: I am a native of Andhra pradesh\n",
      "Item 3: I am passionate about data analytics in various domains Teleco, banking and health care\n",
      "Item 4: I am currnelty focussed on leanring NLP from Coursera\n",
      "Item 5: I admire proff. Gilbert strang as he teaches Maths in a most simplest way.\n",
      "Item 6: You are awesome!\n"
     ]
    }
   ],
   "source": [
    "# Define a custom data set\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "# Tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary and include special tokens\n",
    "special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences), specials=special_tokens)\n",
    "\n",
    "# Set the default index to handle out-of-vocabulary (OOV) tokens\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Create an instance of your custom data set\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "\n",
    "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
    "print(\"Sample Items:\")\n",
    "for i in range(6):\n",
    "    sample_item = custom_dataset[i]\n",
    "    print(f\"Item {i + 1}: {sample_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch splitter -----------------\n",
      "tensor([[ 1,  4,  5, 49, 30, 33,  7, 52,  9,  6, 10, 44, 19, 26,  2],\n",
      "        [ 1,  4,  5,  6, 37, 39, 17, 42,  2,  0,  0,  0,  0,  0,  0]])\n",
      "batch splitter -----------------\n",
      "tensor([[ 1,  4,  5, 41, 13, 10, 15, 11, 50, 25, 48,  7, 21, 16, 32, 22,  2],\n",
      "        [ 1,  4,  5, 24, 27, 40, 34, 38, 28, 23,  2,  0,  0,  0,  0,  0,  0]])\n",
      "batch splitter -----------------\n",
      "tensor([[ 1,  4, 14, 43,  8, 29, 46,  9, 31, 47, 35, 11,  6, 36, 45, 51,  8,  2],\n",
      "        [ 1, 53, 18, 20, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences within the batch to have equal lengths\n",
    "    tensor_batch=[]\n",
    "    for sample in batch:\n",
    "        #basic tokenization\n",
    "        tokens = tokenizer(sample)\n",
    "        #special tokenization\n",
    "        tokens = ['<bos>'] + tokens + ['<eos>']\n",
    "        # Vocabulary Building\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "         \n",
    "    # Dynamic Padding Within Batches\n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
    "    return padded_batch\n",
    "\n",
    "dataloader = DataLoader(custom_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print('batch splitter -----------------')\n",
    "    print(batch)\n",
    "\n",
    "# Note:\n",
    "# dynamic padding is preferred,as the batch is padded only to the length of its longest sequence, \n",
    "# minimizing the number of padding tokens and making training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
