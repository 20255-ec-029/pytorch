{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I am Vamshidhar H K, working as a data scientist at Flytxt\",\n",
    "    \"I am a native of Andhra pradesh\",\n",
    "    \"I am passionate about data analytics in various domains Teleco, banking and health care\",\n",
    "    \"I am currnelty focussed on leanring NLP from Coursera\",\n",
    "    \"I admire proff. Gilbert strang as he teaches Maths in a most simplest way.\",\n",
    "    \"You are awesome!\"\n",
    "]\n",
    "### Tokenization:\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sent in sentences:\n",
    "    tokenized_sentences.append(tokenizer(sent)) # tokenizer requires each sample and not the list of samples.\n",
    "print(f'full data after first tokenization:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences:\n",
    "    print(i)\n",
    "\n",
    "### Adding special tokens:\n",
    "tokenized_sentences_new = []\n",
    "max_length = 0\n",
    "for tokenized_sent in tokenized_sentences:\n",
    "    tokenized_sentence_new = ['<bos>'] + tokenized_sent + ['<eos>']\n",
    "    tokenized_sentences_new.append(tokenized_sentence_new)\n",
    "    max_length = max(max_length, len(tokenized_sentence_new))\n",
    "print(f'max length : {max_length}')\n",
    "print(f'full data after adding <bos> & <eos>:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)\n",
    "### Adding special tokens:\n",
    "tokenized_sentences_new = []\n",
    "max_length = 0\n",
    "for tokenized_sent in tokenized_sentences:\n",
    "    tokenized_sentence_new = ['<bos>'] + tokenized_sent + ['<eos>']\n",
    "    tokenized_sentences_new.append(tokenized_sentence_new)\n",
    "    max_length = max(max_length, len(tokenized_sentence_new))\n",
    "print(f'max length : {max_length}')\n",
    "print(f'full data after adding <bos> & <eos>:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)\n",
    "\n",
    "### It is observed that all have different lengths and max is 18, so lets do padding\n",
    "###  to maintain same length for each sentence.\n",
    "for i in range(len(tokenized_sentences_new)):\n",
    "    tokenized_sentences_new[i] = tokenized_sentences_new[i] + ['<pad>'] * (max_length - len(tokenized_sentences_new[i]))\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)\n",
    "\n",
    "### \"Vocabulary Building\" or \"Vocabulary Construction\":\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<unk>\"]) \n",
    "vocab.set_default_index(vocab[\"<unk>\"]) \n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "# this will give indexs : {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "# It is required to provide unique index to each spl token for the model to differentiate each token separately,\n",
    "# or else it will be problematic when model predicts 0 index, and it is difficult to decide which spl token it is refering to. \n",
    "#: It is a common convention to assign the padding token the index 0, so keep <pad> at first,\n",
    "#: and for rest of the tokens, any number is provided. \n",
    "\n",
    "### Indexing\n",
    "indexed_tokens = []\n",
    "for tokenized_sent in tokenized_sentences_new:\n",
    "    indexed_tokens.append([vocab[x] for x in tokenized_sent])\n",
    "print(f'indexed last sample: {indexed_tokens[5]}')\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "for i in indexed_tokens:\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = [\n",
    "    \"I am Vamshidhar H K, working as a data scientist at Flytxt\",\n",
    "    \"I am a native of Andhra pradesh\",\n",
    "    \"I am passionate about data analytics in various domains Teleco, banking and health care\",\n",
    "    \"I am currnelty focussed on leanring NLP from Coursera\",\n",
    "    \"I admire proff. Gilbert strang as he teaches Maths in a most simplest way.\",\n",
    "    \"You are awesome!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manully processing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full data after first tokenization:---------------------------------------------------------------\n",
      "['i', 'am', 'vamshidhar', 'h', 'k', ',', 'working', 'as', 'a', 'data', 'scientist', 'at', 'flytxt']\n",
      "['i', 'am', 'a', 'native', 'of', 'andhra', 'pradesh']\n",
      "['i', 'am', 'passionate', 'about', 'data', 'analytics', 'in', 'various', 'domains', 'teleco', ',', 'banking', 'and', 'health', 'care']\n",
      "['i', 'am', 'currnelty', 'focussed', 'on', 'leanring', 'nlp', 'from', 'coursera']\n",
      "['i', 'admire', 'proff', '.', 'gilbert', 'strang', 'as', 'he', 'teaches', 'maths', 'in', 'a', 'most', 'simplest', 'way', '.']\n",
      "['you', 'are', 'awesome', '!']\n"
     ]
    }
   ],
   "source": [
    "### Tokenization:\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokenized_sentences = []\n",
    "for sent in sentences:\n",
    "    tokenized_sentences.append(tokenizer(sent)) # tokenizer requires each sample and not the list of samples.\n",
    "print(f'full data after first tokenization:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length : 18\n",
      "full data after adding <bos> & <eos>:---------------------------------------------------------------\n",
      "['<bos>', 'i', 'am', 'vamshidhar', 'h', 'k', ',', 'working', 'as', 'a', 'data', 'scientist', 'at', 'flytxt', '<eos>']\n",
      "['<bos>', 'i', 'am', 'a', 'native', 'of', 'andhra', 'pradesh', '<eos>']\n",
      "['<bos>', 'i', 'am', 'passionate', 'about', 'data', 'analytics', 'in', 'various', 'domains', 'teleco', ',', 'banking', 'and', 'health', 'care', '<eos>']\n",
      "['<bos>', 'i', 'am', 'currnelty', 'focussed', 'on', 'leanring', 'nlp', 'from', 'coursera', '<eos>']\n",
      "['<bos>', 'i', 'admire', 'proff', '.', 'gilbert', 'strang', 'as', 'he', 'teaches', 'maths', 'in', 'a', 'most', 'simplest', 'way', '.', '<eos>']\n",
      "['<bos>', 'you', 'are', 'awesome', '!', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "### Adding special tokens:\n",
    "tokenized_sentences_new = []\n",
    "max_length = 0\n",
    "for tokenized_sent in tokenized_sentences:\n",
    "    tokenized_sentence_new = ['<bos>'] + tokenized_sent + ['<eos>']\n",
    "    tokenized_sentences_new.append(tokenized_sentence_new)\n",
    "    max_length = max(max_length, len(tokenized_sentence_new))\n",
    "print(f'max length : {max_length}')\n",
    "print(f'full data after adding <bos> & <eos>:---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full data after padding :---------------------------------------------------------------\n",
      "['<bos>', 'i', 'am', 'vamshidhar', 'h', 'k', ',', 'working', 'as', 'a', 'data', 'scientist', 'at', 'flytxt', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', 'am', 'a', 'native', 'of', 'andhra', 'pradesh', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', 'am', 'passionate', 'about', 'data', 'analytics', 'in', 'various', 'domains', 'teleco', ',', 'banking', 'and', 'health', 'care', '<eos>', '<pad>']\n",
      "['<bos>', 'i', 'am', 'currnelty', 'focussed', 'on', 'leanring', 'nlp', 'from', 'coursera', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<bos>', 'i', 'admire', 'proff', '.', 'gilbert', 'strang', 'as', 'he', 'teaches', 'maths', 'in', 'a', 'most', 'simplest', 'way', '.', '<eos>']\n",
      "['<bos>', 'you', 'are', 'awesome', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "### It is observed that all have different lengths and max is 18, so lets do padding\n",
    "###  to maintain same length for each sentence.\n",
    "for i in range(len(tokenized_sentences_new)):\n",
    "    tokenized_sentences_new[i] = tokenized_sentences_new[i] + ['<pad>'] * (max_length - len(tokenized_sentences_new[i]))\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "for i in tokenized_sentences_new:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed last sample: [1, 53, 18, 20, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "full data after padding :---------------------------------------------------------------\n",
      "tensor([ 1,  4,  5, 49, 30, 33,  7, 52,  9,  6, 10, 44, 19, 26,  2,  0,  0,  0])\n",
      "tensor([ 1,  4,  5,  6, 37, 39, 17, 42,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 1,  4,  5, 41, 13, 10, 15, 11, 50, 25, 48,  7, 21, 16, 32, 22,  2,  0])\n",
      "tensor([ 1,  4,  5, 24, 27, 40, 34, 38, 28, 23,  2,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 1,  4, 14, 43,  8, 29, 46,  9, 31, 47, 35, 11,  6, 36, 45, 51,  8,  2])\n",
      "tensor([ 1, 53, 18, 20, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "### \"Vocabulary Building\" or \"Vocabulary Construction\":\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<unk>\"]) \n",
    "vocab.set_default_index(vocab[\"<unk>\"]) \n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenized_sentences_new, specials=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "# this will give indexs : {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "# It is required to provide unique index to each spl token for the model to differentiate each token separately,\n",
    "# or else it will be problematic when model predicts 0 index, and it is difficult to decide which spl token it is refering to. \n",
    "#: It is a common convention to assign the padding token the index 0, so keep <pad> at first,\n",
    "#: and for rest of the tokens, any number is provided. \n",
    "\n",
    "# Assign specific indices if required\n",
    "\"\"\"\n",
    "vocab.get_stoi()[\"<pad>\"] = 0\n",
    "vocab.get_stoi()[\"<bos>\"] = 1\n",
    "vocab.get_stoi()[\"<eos>\"] = 2\n",
    "vocab.get_stoi()[\"<unk>\"] = 3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### Indexing\n",
    "indexed_tokens = []\n",
    "for tokenized_sent in tokenized_sentences_new:\n",
    "    indexed_tokens.append([vocab[x] for x in tokenized_sent])\n",
    "print(f'indexed last sample: {indexed_tokens[5]}')\n",
    "\n",
    "print(f'full data after padding :---------------------------------------------------------------' )\n",
    "import torch\n",
    "for i in indexed_tokens:\n",
    "    print(torch.tensor(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CustomDataset class and collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset Length: 6\n",
      "Sample Items:\n",
      "Item 1: I am Vamshidhar H K, working as a data scientist at Flytxt\n",
      "Item 2: I am a native of Andhra pradesh\n",
      "Item 3: I am passionate about data analytics in various domains Teleco, banking and health care\n",
      "Item 4: I am currnelty focussed on leanring NLP from Coursera\n",
      "Item 5: I admire proff. Gilbert strang as he teaches Maths in a most simplest way.\n",
      "Item 6: You are awesome!\n"
     ]
    }
   ],
   "source": [
    "# Define a custom data set\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "# Tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary and include special tokens\n",
    "special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, sentences), specials=special_tokens)\n",
    "\n",
    "# Set the default index to handle out-of-vocabulary (OOV) tokens\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Create an instance of your custom data set\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "\n",
    "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
    "print(\"Sample Items:\")\n",
    "for i in range(6):\n",
    "    sample_item = custom_dataset[i]\n",
    "    print(f\"Item {i + 1}: {sample_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch splitter -----------------\n",
      "tensor([[ 1,  4,  5, 49, 30, 33,  7, 52,  9,  6, 10, 44, 19, 26,  2],\n",
      "        [ 1,  4,  5,  6, 37, 39, 17, 42,  2,  0,  0,  0,  0,  0,  0]])\n",
      "batch splitter -----------------\n",
      "tensor([[ 1,  4,  5, 41, 13, 10, 15, 11, 50, 25, 48,  7, 21, 16, 32, 22,  2],\n",
      "        [ 1,  4,  5, 24, 27, 40, 34, 38, 28, 23,  2,  0,  0,  0,  0,  0,  0]])\n",
      "batch splitter -----------------\n",
      "tensor([[ 1,  4, 14, 43,  8, 29, 46,  9, 31, 47, 35, 11,  6, 36, 45, 51,  8,  2],\n",
      "        [ 1, 53, 18, 20, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences within the batch to have equal lengths\n",
    "    tensor_batch=[]\n",
    "    for sample in batch:\n",
    "        #basic tokenization\n",
    "        tokens = tokenizer(sample)\n",
    "        #special tokenization\n",
    "        tokens = ['<bos>'] + tokens + ['<eos>']\n",
    "        # Vocabulary Building\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "         \n",
    "    # Dynamic Padding Within Batches\n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
    "    return padded_batch\n",
    "\n",
    "dataloader = DataLoader(custom_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print('batch splitter -----------------')\n",
    "    print(batch)\n",
    "\n",
    "# Note:\n",
    "# dynamic padding is preferred,as the batch is padded only to the length of its longest sequence, \n",
    "# minimizing the number of padding tokens and making training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch splitter -----------------\n",
      "tensor([[ 1,  1],\n",
      "        [ 4,  4],\n",
      "        [ 5,  5],\n",
      "        [49,  6],\n",
      "        [30, 37],\n",
      "        [33, 39],\n",
      "        [ 7, 17],\n",
      "        [52, 42],\n",
      "        [ 9,  2],\n",
      "        [ 6,  0],\n",
      "        [10,  0],\n",
      "        [44,  0],\n",
      "        [19,  0],\n",
      "        [26,  0],\n",
      "        [ 2,  0]])\n",
      "batch splitter -----------------\n",
      "tensor([[ 1,  1],\n",
      "        [ 4,  4],\n",
      "        [ 5,  5],\n",
      "        [41, 24],\n",
      "        [13, 27],\n",
      "        [10, 40],\n",
      "        [15, 34],\n",
      "        [11, 38],\n",
      "        [50, 28],\n",
      "        [25, 23],\n",
      "        [48,  2],\n",
      "        [ 7,  0],\n",
      "        [21,  0],\n",
      "        [16,  0],\n",
      "        [32,  0],\n",
      "        [22,  0],\n",
      "        [ 2,  0]])\n",
      "batch splitter -----------------\n",
      "tensor([[ 1,  1],\n",
      "        [ 4, 53],\n",
      "        [14, 18],\n",
      "        [43, 20],\n",
      "        [ 8, 12],\n",
      "        [29,  2],\n",
      "        [46,  0],\n",
      "        [ 9,  0],\n",
      "        [31,  0],\n",
      "        [47,  0],\n",
      "        [35,  0],\n",
      "        [11,  0],\n",
      "        [ 6,  0],\n",
      "        [36,  0],\n",
      "        [45,  0],\n",
      "        [51,  0],\n",
      "        [ 8,  0],\n",
      "        [ 2,  0]])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn_nobatchfirst(batch):\n",
    "    # Pad sequences within the batch to have equal lengths\n",
    "    tensor_batch=[]\n",
    "    for sample in batch:\n",
    "        #basic tokenization\n",
    "        tokens = tokenizer(sample)\n",
    "        #special tokenization\n",
    "        tokens = ['<bos>'] + tokens + ['<eos>']\n",
    "        # Vocabulary Building\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "         \n",
    "    # Dynamic Padding Within Batches\n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=False)\n",
    "    return padded_batch\n",
    "\n",
    "dataloader = DataLoader(custom_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn_nobatchfirst)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print('batch splitter -----------------')\n",
    "    print(batch)\n",
    "\n",
    "# Note:\n",
    "# dynamic padding is preferred,as the batch is padded only to the length of its longest sequence, \n",
    "# minimizing the number of padding tokens and making training more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `batch_first=True` vs `batch_first=False`                             \n",
    "Batch Size: The number of sequences (or samples) in a batch.                                             \n",
    "Sequence Length: The length of each sequence (e.g., number of words in a sentence).                   \n",
    "Num Features: The size of each feature vector (e.g., embedding dimension).\n",
    "\n",
    "\n",
    "Use `batch_first=True` size: (batch_size, sequence_length, num_features)\n",
    "You prefer a more intuitive way to work with batches of data in the form (batch_size, sequence_length, num_features).\n",
    "This layout is easier to reason about, especially if you often think of the data in terms of batches of sequences.\n",
    "Your downstream processing (e.g., fully connected layers) expects the batch size as the first dimension.\n",
    "\n",
    "Common in NLP Tasks: For tasks like text classification, machine translation, and language modeling, it is more intuitive to arrange the input as (batch_size, sequence_length, num_features) since each batch contains a collection of sentences or sequences.\n",
    "\n",
    "Compatibility with Certain Layers:\n",
    "Many PyTorch layers support this option.\n",
    "Feedforward Layers: When passing data from an RNN or Transformer to a fully connected layer (nn.Linear), having batch_first=True aligns naturally with the way many dense layers expect the input tensor.\n",
    "Padding and Masking: Some implementations of padding and masking (e.g., sequence masking in Transformers) often operate more naturally when the input is in the shape (batch_size, sequence_length, num_features).\n",
    "\n",
    "Use `batch_first=False` (default) size: (sequence_length, batch_size, num_features)\n",
    "when you need your data need to be like this (sequence_length, batch_size, num_features)\n",
    "You work with sequence data that is naturally represented with time steps first.\n",
    "You want to align with the default behavior of many recurrent layers in PyTorch.\n",
    "You are working on time-series models that require direct iteration over time steps.\n",
    "The default in many PyTorch functions and modules, especially in recurrent layers like torch.nn.LSTM and torch.nn.GRU (if batch_first is not set).\n",
    "Useful when directly working with time series data where the time steps come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
