{"cells":[{"cell_type":"markdown","metadata":{},"source":["## PyTorch/Embedding and EmbeddingBag"]},{"cell_type":"markdown","metadata":{},"source":["Embedding is a class that represents an embedding layer. It accepts token indices and produces embedding vectors. EmbeddingBag is a class that aggregates embeddings using mean or sum operations. Embedding and EmbeddingBag are part of the torch.nn module. The code example shows how you can use Embedding and EmbeddingBag in PyTorch."]},{"cell_type":"markdown","metadata":{},"source":["`torch:` The core PyTorch library used for building and training deep learning models.                          \n","`torch.nn:` A module in PyTorch that contains classes for building neural network layers.                           \n","`tensor:` A multi-dimensional matrix containing elements of a single data type, essential for PyTorch computations."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","import torch"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[tensor([0, 7, 2]), tensor([0, 4, 3]), tensor([0, 1, 6, 8, 5])]\n"]}],"source":["# Defining a data set\n","dataset = [\n","\"I like cats\",\n","\"I hate dogs\",\n","\"I'm impartial to hippos\"\n","]\n","#Initializing the tokenizer, iterator from the data set, and vocabulary\n","tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n","def yield_tokens(data_iter):\n","    for data_sample in data_iter:\n","        yield tokenizer(data_sample)\n","data_iter = iter(dataset)\n","vocab = build_vocab_from_iterator(yield_tokens(data_iter))\n","#Tokenizing and generating indices\n","input_ids=lambda x:[torch.tensor(vocab(tokenizer(data_sample))) for data_sample in dataset]\n","index=input_ids(dataset)\n","print(index)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["embeddings of i_like_cats: tensor([[ 0.0696, -0.0102, -1.1019],\n","        [ 0.7790, -0.9232, -0.1057],\n","        [ 0.5286,  1.6652,  0.1281]], grad_fn=<EmbeddingBackward0>)\n","embeddings of i_hate_dogs: tensor([[ 0.0696, -0.0102, -1.1019],\n","        [ 0.9367, -2.2018,  0.7229],\n","        [ 0.4811, -1.5437, -0.5988]], grad_fn=<EmbeddingBackward0>)\n","embeddings of impartial_to_hippos: tensor([[ 0.0696, -0.0102, -1.1019],\n","        [-0.8129,  0.5123,  1.2359],\n","        [ 1.2870, -1.0623, -0.3928],\n","        [-0.5383,  0.1056,  1.2875],\n","        [ 0.7156,  0.2015, -0.1643]], grad_fn=<EmbeddingBackward0>)\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch import tensor\n","# import torch.nn.functional as F\n","\n","#Initiating the embedding layer, specifying the dimension size for the embeddings, \n","#determining the count of unique tokens present in the vocabulary, and creating the embedding layer\n","embedding_dim = 3\n","n_embedding = len(vocab) #9\n","embeds = nn.Embedding(n_embedding, embedding_dim)\n","#Applying the embedding object\n","i_like_cats=embeds(index[0])\n","print(f'embeddings of i_like_cats: {i_like_cats}')\n","\n","i_hate_dogs=embeds(index[1])\n","print(f'embeddings of i_hate_dogs: {i_hate_dogs}')\n","\n","impartial_to_hippos=embeds(index[-1])\n","print(f'embeddings of impartial_to_hippos: {impartial_to_hippos}')"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["embeddings of i_like_cats: tensor([[-0.0531,  1.1954,  0.8458]], grad_fn=<EmbeddingBagBackward0>)\n","embeddings of i_hate_dogs: tensor([[0.6154, 0.3378, 0.1228]], grad_fn=<EmbeddingBagBackward0>)\n","embeddings of impartial_to_hippos: tensor([[ 0.8911, -0.0842,  0.5150]], grad_fn=<EmbeddingBagBackward0>)\n"]}],"source":["#Initiating the embedding layer, specifying the dimension size for the embeddings, \n","#determining the count of unique tokens present in the vocabulary, and creating the embedding layer\n","embedding_dim = 3\n","n_embedding = len(vocab) #9\n","embedding_bag = nn.EmbeddingBag(n_embedding, embedding_dim)\n","\n","#Applying the embedding object\n","print(f'embeddings of i_like_cats: {embedding_bag(index[0],offsets=torch.tensor([0]))}')\n","\n","print(f'embeddings of i_hate_dogs: {embedding_bag(index[1],offsets=torch.tensor([0]))}')\n","\n","print(f'embeddings of impartial_to_hippos: {embedding_bag(index[-1],offsets=torch.tensor([0]))}')"]},{"cell_type":"markdown","metadata":{},"source":["Explanation of Modes in EmbeddingBag                                                                     \n","`embedding_bag = nn.EmbeddingBag(num_embeddings, embedding_dim, mode='mean')`\n","\n","The mode parameter in nn.EmbeddingBag defines how the embeddings of the tokens in each \"bag\" (or sequence) are aggregated. The available options are:                                                \n","'sum': Sums the embeddings of all tokens in the bag.                                        \n","'mean': Averages the embeddings of all tokens in the bag. It is default                                  \n","'max': Takes the maximum value for each dimension across all token embeddings in the bag.                      "]},{"cell_type":"markdown","metadata":{},"source":["`Need of offsets:`                                        \n","Especially incase of n grams, we usually concatinate the sentens based on the model configs, for example for 3 gram model we combine first 2 words as a context and 3rd word as a target word. \n","imagine if indexes of these concatinated words are [0, 1] and [2, 3, 4]\n","so in this case, we can embedd these two words at the same time. using below code"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.7735,  1.2749,  0.8850],\n","        [ 0.7659, -0.3560,  0.3488]], grad_fn=<EmbeddingBagBackward0>)\n"]}],"source":["# New indices representing two sequences: [0, 1] and [2, 3, 4]\n","indices = torch.tensor([0, 1, 2, 3, 4])  # Concatenate indices of all sequences\n","\n","# Offsets to mark the start of each sequence in `indices`\n","offsets = torch.tensor([0, 2])  # First sequence starts at index 0, second starts at index 2\n","\n","output = embedding_bag(indices, offsets)\n","print(output)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Batch function                                                               \n","Defines the number of samples that will be propagated through the network."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def collate_batch(batch):\n","    target_list, context_list, offsets = [], [], [0]\n","    for _context, _target in batch:\n","        target_list.append(vocab[_target]) \n","        processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)\n","        context_list.append(processed_context)\n","        offsets.append(processed_context.size(0))\n","        target_list = torch.tensor(target_list, dtype=torch.int64)\n","        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","        context_list = torch.cat(context_list)\n","    return target_list.to(device), context_list.to(device), offsets.to(device)\n","\n","BATCH_SIZE = 64 # batch size for training\n","dataloader_cbow = DataLoader(cobw_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"}},"nbformat":4,"nbformat_minor":2}
