{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791ffce-6e3d-400d-8b9c-bdbf4432734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim #4.2.0\n",
    "!pip install portalocker>=2.0.0\n",
    "!pip install -Uq torchtext==0.15.2\n",
    "!pip install -Uq torch==2.0.1\n",
    "!pip install -Uq torchdata==0.6.1\n",
    "!pip install -Uq pandas\n",
    "!pip install -Uq matplotlib\n",
    "!pip install -Uq seaborn\n",
    "!pip install -Uq scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672fb7c0-bb1c-4f80-a7e8-0b72e9d62348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.core.display import display, SVG\n",
    "\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe,vocab\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5f51b-a3f6-4abb-8b43-e652e7a5defe",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81211e-ea3f-4b71-942f-2b31fa167ae7",
   "metadata": {},
   "source": [
    "# CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0660b53e-178c-4904-9823-f3b5fac7a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = \"\"\"I wish I was little bit taller\n",
    "I wish I was a baller\n",
    "She wore a small black dress to the party\n",
    "The dog chased a big red ball in the park\n",
    "He had a huge smile on his face when he won the race\n",
    "The tiny kitten played with a fluffy toy mouse\n",
    "The team celebrated their victory with a grand parade\n",
    "She bought a small, delicate necklace for her sister\n",
    "The mountain peak stood majestic and tall against the clear blue sky\n",
    "The toddler took small, careful steps as she learned to walk\n",
    "The house had a spacious backyard with a big swimming pool\n",
    "He felt a sense of accomplishment after completing the challenging puzzle\n",
    "The chef prepared a delicious, flavorful dish using fresh ingredients\n",
    "The children played happily in the small, cozy room\n",
    "The book had an enormous impact on readers around the world\n",
    "The wind blew gently, rustling the leaves of the tall trees\n",
    "She painted a beautiful, intricate design on the small canvas\n",
    "The concert hall was filled with thousands of excited fans\n",
    "The garden was adorned with colorful flowers of all sizes\n",
    "I hope to achieve great success in my chosen career path\n",
    "The skyscraper towered above the city, casting a long shadow\n",
    "He gazed in awe at the breathtaking view from the mountaintop\n",
    "The artist created a stunning masterpiece with bold brushstrokes\n",
    "The baby took her first steps, a small milestone that brought joy to her parents\n",
    "The team put in a tremendous amount of effort to win the championship\n",
    "The sun set behind the horizon, painting the sky in vibrant colors\n",
    "The professor gave a fascinating lecture on the history of ancient civilizations\n",
    "The house was filled with laughter and the sound of children playing\n",
    "She received a warm, enthusiastic welcome from the audience\n",
    "The marathon runner had incredible endurance and determination\n",
    "The child's eyes sparkled with excitement upon opening the gift\n",
    "The ship sailed across the vast ocean, guided by the stars\n",
    "The company achieved remarkable growth in a short period of time\n",
    "The team worked together harmoniously to complete the project\n",
    "The puppy wagged its tail, expressing its happiness and affection\n",
    "She wore a stunning gown that made her feel like a princess\n",
    "The building had a grand entrance with towering columns\n",
    "The concert was a roaring success, with the crowd cheering and clapping\n",
    "The baby took a tiny bite of the sweet, juicy fruit\n",
    "The athlete broke a new record, achieving a significant milestone in her career\n",
    "The sculpture was a masterpiece of intricate details and craftsmanship\n",
    "The forest was filled with towering trees, creating a sense of serenity\n",
    "The children built a small sandcastle on the beach, their imaginations running wild\n",
    "The mountain range stretched as far as the eye could see, majestic and awe-inspiring\n",
    "The artist's brush glided smoothly across the canvas, creating a beautiful painting\n",
    "She received a small token of appreciation for her hard work and dedication\n",
    "The orchestra played a magnificent symphony that moved the audience to tears\n",
    "The flower bloomed in vibrant colors, attracting butterflies and bees\n",
    "The team celebrated their victory with a big, extravagant party\n",
    "The child's laughter echoed through the small room, filling it with joy\n",
    "The sunflower stood tall, reaching for the sky with its bright yellow petals\n",
    "The city skyline was dominated by tall buildings and skyscrapers\n",
    "The cake was adorned with a beautiful, elaborate design for the special occasion\n",
    "The storm brought heavy rain and strong winds, causing widespread damage\n",
    "The small boat sailed peacefully on the calm, glassy lake\n",
    "The artist used bold strokes of color to create a striking and vivid painting\n",
    "The couple shared a passionate kiss under the starry night sky\n",
    "The mountain climber reached the summit after a long and arduous journey\n",
    "The child's eyes widened in amazement as the magician performed his tricks\n",
    "The garden was filled with the sweet fragrance of blooming flowers\n",
    "The basketball player made a big jump and scored a spectacular slam dunk\n",
    "The cat pounced on a small mouse, displaying its hunting instincts\n",
    "The mansion had a grand entrance with a sweeping staircase and chandeliers\n",
    "The raindrops fell gently, creating a rhythmic patter on the roof\n",
    "The baby took a big step forward, encouraged by her parents' applause\n",
    "The actor delivered a powerful and emotional performance on stage\n",
    "The butterfly fluttered its delicate wings, mesmerizing those who watched\n",
    "The company launched a small-scale advertising campaign to test the market\n",
    "The building was constructed with strong, sturdy materials to withstand earthquakes\n",
    "The singer's voice was powerful and resonated throughout the concert hall\n",
    "The child built a massive sandcastle with towers, moats, and bridges\n",
    "The garden was teeming with a variety of small insects and buzzing bees\n",
    "The athlete's muscles were well-developed and strong from years of training\n",
    "The sun cast long shadows as it set behind the mountains\n",
    "The couple exchanged heartfelt vows in a beautiful, intimate ceremony\n",
    "The dog wagged its tail vigorously, a sign of excitement and happiness\n",
    "The baby let out a tiny giggle, bringing joy to everyone around\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2f5bb-f55c-4233-b189-366269c4df99",
   "metadata": {},
   "source": [
    "# Manual processing to get context and target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502ca0b-9e7c-4294-844c-09b2d33dd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')  # This uses basic English tokenizer. You can choose another.\n",
    "\n",
    "# Step 2: Tokenize sentences\n",
    "def tokenize_data(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "tokenized_toy_data = tokenizer (toy_data)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenize_data(tokenized_toy_data), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a072a-b762-491b-94a1-3e746644a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "sample_sentence = \"I wish I was a baller\"\n",
    "tokenized_sample = tokenizer(sample_sentence)\n",
    "encoded_sample = [vocab[token] for token in tokenized_sample]\n",
    "print(\"Encoded sample:\", encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf39c61-0849-4d8e-85bd-22f200f711cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda tokens:[ vocab[token]  for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a78d7-25c5-4abe-b2b3-ec427e294ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE=2\n",
    "\n",
    "cobow_data = []\n",
    "for i in range(1, len(tokenized_toy_data ) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [tokenized_toy_data [i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [tokenized_toy_data [i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = tokenized_toy_data [i]\n",
    "    cobow_data.append((context, target))\n",
    "    \n",
    "print(cobow_data[1])\n",
    "print(cobow_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c21982-4a28-4e60-8756-589953186536",
   "metadata": {},
   "source": [
    "## Rest of the pre-processing using Collate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c751d-e309-48d2-ae7d-129d6e2a36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    target_list, context_list, offsets = [], [], [0]\n",
    "    for _context, _target in batch:\n",
    "        \n",
    "        target_list.append(vocab[_target])  \n",
    "        processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)\n",
    "        context_list.append(processed_context)\n",
    "        offsets.append(processed_context.size(0))\n",
    "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    context_list = torch.cat(context_list)\n",
    "    return target_list.to(device), context_list.to(device), offsets.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "target_list, context_list, offsets=collate_batch(cobow_data[0:10])\n",
    "print(f\"target_list(Tokenized target words): {target_list} , context_list(Surrounding context words): {context_list} , offsets(Starting indexes of context words for each target): {offsets} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38d55a-1ade-4998-baa5-2286f021c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "dataloader_cbow = DataLoader(\n",
    "    cobow_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "print(dataloader_cbow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19400668-32f3-439a-a992-62a300716b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    # Initialize the CBOW model\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        \n",
    "        super(CBOW, self).__init__()\n",
    "         # Define the embedding layer using nn.EmbeddingBag\n",
    "        # It outputs the average of context words embeddings\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        # Define the first linear layer with input size embed_dim and output size embed_dim//2\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim//2)\n",
    "        # Define the fully connected layer with input size embed_dim//2 and output size vocab_size\n",
    "        self.fc = nn.Linear(embed_dim//2, vocab_size)\n",
    "        \n",
    "\n",
    "        self.init_weights()\n",
    "    # Initialize the weights of the model's parameters\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of the embedding layer\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialize the weights of the fully connected layer\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        # Initialize the biases of the fully connected layer to zeros\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Pass the input text and offsets through the embedding layer\n",
    "        out = self.embedding(text, offsets)\n",
    "        # Apply the ReLU activation function to the output of the first linear layer\n",
    "        out = torch.relu(self.linear1(out))\n",
    "        # Pass the output of the ReLU activation through the fully connected layer\n",
    "        return self.fc(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cda45b-30c1-46d9-a642-8c4595a30685",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emsize = 24\n",
    "model_cbow = CBOW(vocab_size, emsize, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71f70b-b99e-4aaa-bbd7-eed588b51a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 5  # learning rate\n",
    "\n",
    "# Define the CrossEntropyLoss criterion. It is commonly used for multi-class classification tasks.\n",
    "# This criterion combines the softmax function and the negative log-likelihood loss.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer using stochastic gradient descent (SGD).\n",
    "# It optimizes the parameters of the model_cbow, which are obtained by model_cbow.parameters().\n",
    "# The learning rate (lr) determines the step size for parameter updates during optimization.\n",
    "optimizer = torch.optim.SGD(model_cbow.parameters(), lr=LR)\n",
    "\n",
    "# Define a learning rate scheduler.\n",
    "# The StepLR scheduler adjusts the learning rate during training.\n",
    "# It multiplies the learning rate by gamma every step_size epochs (here, 1.0).\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b72f99-d73a-4f13-a179-1561a22834f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train the model for the specified number of epochs.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to be trained.\n",
    "        dataloader: DataLoader providing data for training.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer for updating model's weights.\n",
    "        num_epochs: Number of epochs to train the model for.\n",
    "\n",
    "    Returns:\n",
    "        model: The trained model.\n",
    "        epoch_losses: List of average losses for each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to store running loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Storing running loss values for the current epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Using tqdm for a progress bar\n",
    "        for idx, samples in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Check for EmbeddingBag layer in the model\n",
    "            if any(isinstance(module, nn.EmbeddingBag) for _, module in model.named_modules()):\n",
    "                target, context, offsets = samples\n",
    "                predicted = model(context, offsets)\n",
    "            \n",
    "            # Check for Embedding layer in the model\n",
    "            elif any(isinstance(module, nn.Embedding) for _, module in model.named_modules()):\n",
    "                target, context = samples\n",
    "                predicted = model(context)\n",
    "                \n",
    "            loss = criterion(predicted, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Append average loss for the epoch\n",
    "        epoch_losses.append(running_loss / len(dataloader))\n",
    "    \n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678b46c-7255-43d4-83b3-f65a8fe65b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow, epoch_losses=train_model(model_cbow, dataloader_cbow, criterion, optimizer, num_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4be9c-c260-4e35-9bc1-774f292dfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67141fd1-bfc0-465f-82d7-310a89876aab",
   "metadata": {},
   "source": [
    "### Obtaining Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88927b-8165-4eee-a28f-0dae3094f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model_cbow.embedding.weight.detach().cpu().numpy() \n",
    "word = 'baller'\n",
    "word_index = vocab.get_stoi()[word] # getting the index of the word in the vocab\n",
    "print(word_embeddings[word_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636c7bf-3112-41de-910e-84638c00d8b3",
   "metadata": {},
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f86465-4cc1-4d6f-bf8f-c95e36afb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window size for the context around the target word.\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Initialize an empty list to store the (target, context) pairs.\n",
    "skip_data = []\n",
    "\n",
    "# Iterate over each word in the tokenized toy_data, while excluding the first \n",
    "# and last few words determined by the CONTEXT_SIZE.\n",
    "for i in range(CONTEXT_SIZE, len(tokenized_toy_data) - CONTEXT_SIZE):\n",
    "\n",
    "    # For a word at position i, the context comprises of words from the preceding CONTEXT_SIZE\n",
    "    # as well as from the succeeding CONTEXT_SIZE. The context words are collected in a list.\n",
    "    context = (\n",
    "        [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)]  # Preceding words\n",
    "        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]  # Succeeding words\n",
    "    )\n",
    "\n",
    "    # The word at the current position i is taken as the target.\n",
    "    target = tokenized_toy_data[i]\n",
    "\n",
    "    # Append the (target, context) pair to the skip_data list.\n",
    "    skip_data.append((target, context))\n",
    "\n",
    "skip_data_=[[(sample[0],word) for word in  sample[1]] for sample in skip_data]\n",
    "skip_data_flat= [item  for items in  skip_data_ for item in items]\n",
    "skip_data_flat[8:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2d8b0-f9f7-4b1a-b44f-29610898ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    target_list, context_list = [], []\n",
    "    for _context, _target in batch:\n",
    "        \n",
    "        target_list.append(vocab[_target]) \n",
    "        context_list.append(vocab[_context])\n",
    "        \n",
    "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "    context_list = torch.tensor(context_list, dtype=torch.int64)\n",
    "    return target_list.to(device), context_list.to(device)\n",
    "dataloader = DataLoader(skip_data_flat, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a8409-0f38-42ab-a1ff-c1bf849fe3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        # Define the embeddings layer\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(in_features=embed_dim, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Perform the forward pass\n",
    "        # Pass the input text through the embeddings layer\n",
    "        out = self.embeddings(text)\n",
    "        \n",
    "        # Pass the output of the embeddings layer through the fully connected layer\n",
    "        # Apply the ReLU activation function\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f61788-447e-4b14-8daf-ae10380d2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 24\n",
    "model_sg = SkipGram_Model(vocab_size, emsize).to(device)\n",
    "\n",
    "LR = 5  # learning rate\n",
    "#BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_sg.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "model_sg, epoch_losses=train_model(model_sg, dataloader, criterion, optimizer, num_epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a21e9-a6d6-431e-9571-8ceb616b56ab",
   "metadata": {},
   "source": [
    "### Obtaining Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ce8ab-058a-4554-8a5b-c775386bf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model_sg.embeddings.weight.detach().cpu().numpy() \n",
    "plot_embeddings(word_embeddings,vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e0f07-ad7d-442c-9ee0-0f7989005ba8",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cacd11-fee4-4730-a008-8ee7f5d68dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of the 6B version of Glove() model\n",
    "glove_vectors_6B = GloVe(name ='6B') # you can specify the model with the following format: GloVe(name='840B', dim=300)\n",
    "\n",
    "# creating another instance of a bigger Glove() model\n",
    "#glove_vectors_840B = GloVe()\n",
    "\n",
    "# load the glove model pretrained weights into a PyTorch embedding layer\n",
    "embeddings_Glove6B = torch.nn.Embedding.from_pretrained(glove_vectors_6B.vectors,freeze=True)\n",
    "\n",
    "word_to_index = glove_vectors_6B.stoi  # Vocabulary index mapping\n",
    "word_to_index['team']\n",
    "\n",
    "embeddings_Glove6B.weight[word_to_index['team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba21b0-b189-4bb0-8f2a-9f84d58c784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an array of example words\n",
    "words = [\n",
    "    \"taller\",\n",
    "    \"short\",\n",
    "    \"black\",\n",
    "    \"white\",\n",
    "    \"dress\",\n",
    "    \"pants\",\n",
    "    \"big\",\n",
    "    \"small\",\n",
    "    \"red\",\n",
    "    \"blue\",\n",
    "    \"smile\",\n",
    "    \"frown\",\n",
    "    \"race\",\n",
    "    \"stroll\",\n",
    "    \"tiny\",\n",
    "    \"huge\",\n",
    "    \"soft\",\n",
    "    \"rough\",\n",
    "    \"team\",\n",
    "    \"individual\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd20ee6-4543-41aa-8073-33efb6821457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dict_Glove6B = {}\n",
    "for word in words:\n",
    "    # Get the index of the word from the vocabulary to access its embedding\n",
    "    embedding_vector = embeddings_Glove6B.weight[word_to_index[word]]\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be skipped.\n",
    "        # add the embedding vector of word to the embedding_dict_Glove6B\n",
    "        embedding_dict_Glove6B[word] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb0628-b590-4207-8176-089be7788d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to find similar words\n",
    "target_word = \"small\"\n",
    "top_k=2\n",
    "similar_words = find_similar_words(target_word, embedding_dict_Glove6B, top_k)\n",
    "\n",
    "# Print the similar words\n",
    "print(\"{} most similar words to {}:\".format(top_k,target_word) ,similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fed040-9411-411a-82f7-3e889fd4d178",
   "metadata": {},
   "source": [
    "# Train a word2vec model from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc65cd-b1cb-4b5e-a32d-39d23e117021",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[\"I\", \"like\", \"to\", \"eat\", \"pizza\"],\n",
    "             [\"Pizza\", \"is\", \"my\", \"favorite\", \"food\"],\n",
    "             [\"I\", \"enjoy\", \"eating\", \"pasta\"]]\n",
    "sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "# Create an instance of Word2Vec model\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Build vocab using the training data\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "# Train the model on your training data\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "# Finding similar words\n",
    "similar_words = w2v_model.wv.most_similar(\"pizza\")\n",
    "print(\"Similar words to 'pizza':\", similar_words)\n",
    "\n",
    "# Calculating word similarity\n",
    "similarity = w2v_model.wv.similarity(\"pizza\", \"pasta\")\n",
    "print(\"Similarity between 'pizza' and 'pasta':\", similarity)\n",
    "\n",
    "# Extract word vectors and create word-to-index mapping\n",
    "word_vectors = w2v_model.wv\n",
    "# a dictionary to map words to their index in vocab\n",
    "word_to_index = {word: index for index, word in enumerate(word_vectors.index_to_key)}\n",
    "\n",
    "# Create an instance of nn.Embedding and load it with the trained vectors\n",
    "embedding_dim = w2v_model.vector_size\n",
    "embedding = torch.nn.Embedding(len(word_vectors.index_to_key), embedding_dim)\n",
    "embedding.weight.data.copy_(torch.from_numpy(word_vectors.vectors))\n",
    "\n",
    "# Example usage: get the embedding for a word\n",
    "word = \"pizza\"\n",
    "word_index = word_to_index[word]\n",
    "word_embedding = embedding(torch.LongTensor([word_index]))\n",
    "print(f\"Word: {word}, Embedding: {word_embedding.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb777b-a1b8-4fed-a41a-1415d5be1eb5",
   "metadata": {},
   "source": [
    "# Text classification using pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffa822-be00-4e86-b833-34110904f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab\n",
    "# Build vocab from glove_vectors\n",
    "# vocab(ordered_dict: Dict, min_freq: int = 1, specials: Optional[List[str]] = None)\n",
    "vocab = vocab(glove_vectors_6B.stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee872d-72c1-47e9-87a5-851c40e2bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab([\"<unk>\",\"Hello\",\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29510b1-772c-449f-8168-42308a91b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# Define functions to process text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36232299-d37f-49bf-8d7c-ff4a71e9c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing iterators.\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# Convert the training and testing iterators to map-style datasets.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Determine the number of samples to be used for training and validation (5% for validation).\n",
    "num_train = int(len(train_dataset) * 0.85)\n",
    "\n",
    "# Randomly split the training dat aset into training and validation datasets using `random_split`.\n",
    "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6abee-f23f-426c-8e35-aa39f9ac76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class labels\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "'''ag_news_label[y]'''\n",
    "num_class = len(set([label for (label, text) in train_iter ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531016f-5e87-4352-a330-56e953beddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    x=x.lower()# you need this as your vocab is in lower case\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1\n",
    "\n",
    "# create label, text and offset for each batch of data\n",
    "# text is the concatenated text for all text data in the batch\n",
    "# you need to have the offsets(the end of text index) for later when you separate texts and predict their label\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "label, text, offsets=next(iter(train_dataloader ))\n",
    "print(label, text, offsets)\n",
    "label.shape, text.shape, offsets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410bb6d-7c82-47c2-95aa-3d5368809f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(glove_vectors_6B.vectors,freeze=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text,offsets):\n",
    "        embedded = self.embedding(text)\n",
    "        # you get the average of word embeddings in the text\n",
    "        means = []\n",
    "        for i in range(1,len(offsets)):\n",
    "            #this is like eme\n",
    "          text_tmp = embedded[offsets[i-1]:offsets[i]]\n",
    "          means.append(text_tmp.mean(0))\n",
    "\n",
    "        return self.fc(torch.stack(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61b34b-c931-4b8f-9226-71f5c69df2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59009100-90a1-4353-b70b-e85464d0fe60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
