{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\"\n",
    "token = word_tokenize(text)\n",
    "print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install spacy transformers sentencepiece\n",
    "# # !pip3 install spacy\n",
    "# # !pip3 install transformers\n",
    "# # !pip3 install sentencepiece\n",
    "\n",
    "# # !pip3 install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.', 'I', 'could', \"n't\", 'see', 'it', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization', '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/pytorch/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['笆！BM', '笆》aught', '笆［e', '笆》oken', 'ization', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "0.15.2+cpu\n",
      "0.6.1\n"
     ]
    }
   ],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "print(torch.__version__)       \n",
    "print(torchtext.__version__)   \n",
    "print(torchdata.__version__)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Introduction to NLP\n",
      "1\n",
      "Introduction to NLP\n",
      "2\n",
      "Basics of PyTorch\n",
      "1\n",
      "NLP Techniques for Text Classification\n",
      "3\n",
      "Named Entity Recognition with PyTorch\n",
      "3\n",
      "Sentiment Analysis using PyTorch\n",
      "3\n",
      "Machine Translation with PyTorch\n",
      "1\n",
      "NLP Named Entity,Sentiment Analysis, Machine Translation\n",
      "1\n",
      "Machine Translation with NLP\n",
      "1\n",
      "Named Entity vs Sentiment Analysis NLP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vs': 21,\n",
       " 'to': 19,\n",
       " 'of': 15,\n",
       " 'introduction': 14,\n",
       " 'recognition': 16,\n",
       " 'for': 13,\n",
       " 'nlp': 1,\n",
       " 'entity': 4,\n",
       " 'pytorch': 2,\n",
       " 'translation': 8,\n",
       " 'techniques': 17,\n",
       " 'machine': 5,\n",
       " 'named': 6,\n",
       " ',': 10,\n",
       " 'text': 18,\n",
       " 'sentiment': 7,\n",
       " '<unk>': 0,\n",
       " 'with': 9,\n",
       " 'basics': 11,\n",
       " 'using': 20,\n",
       " 'analysis': 3,\n",
       " 'classification': 12}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# Defines a dataset\n",
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\"NLP Named Entity,Sentiment Analysis, Machine Translation\"),\n",
    "    (1,\"Machine Translation with NLP\"),\n",
    "    (1,\"Named Entity vs Sentiment Analysis NLP\")]\n",
    "# Applies the tokenizer to the text to get the tokens as a list\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(dataset[0][1])\n",
    "# Takes a data iterator as input, processes text from the iterator, \n",
    "# and yields the tokenized output individually\n",
    "def yield_tokens(data_iter):\n",
    "    for _,text in data_iter:\n",
    "        print(_)\n",
    "        print(text)\n",
    "        yield tokenizer(text)\n",
    "# Creates an iterator\n",
    "my_iterator = yield_tokens(dataset)\n",
    "# Fetches the next set of tokens from the data set\n",
    "next(my_iterator)\n",
    "# Converts tokens to indices and sets <unk> as the \n",
    "# default word if a word is not found in the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# Gives a dictionary that maps words to their corresponding numerical indices\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Basics of PyTorch\n",
      "1\n",
      "NLP Techniques for Text Classification\n",
      "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
      "Token Indices: [11, 15, 2]\n"
     ]
    }
   ],
   "source": [
    "# Takes an iterator as input and extracts the next tokenized sentence. \n",
    "# Creates a list of token indices using the vocab dictionary for each token.\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices\n",
    "# Returns the tokenized sentences and the corresponding token indices. \n",
    "# Repeats the process.\n",
    "tokenized_sentence, token_indices = \\\n",
    "get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "# Prints the tokenized sentence and its corresponding token indices.\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Named Entity Recognition with PyTorch\n",
      "3\n",
      "Sentiment Analysis using PyTorch\n",
      "Tokenized Sentence: ['named', 'entity', 'recognition', 'with', 'pytorch']\n",
      "Token Indices: [6, 4, 16, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "# Takes an iterator as input and extracts the next tokenized sentence. \n",
    "# Creates a list of token indices using the vocab dictionary for each token.\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices\n",
    "# Returns the tokenized sentences and the corresponding token indices. \n",
    "# Repeats the process.\n",
    "tokenized_sentence, token_indices = \\\n",
    "get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "# Prints the tokenized sentence and its corresponding token indices.\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends <bos> at the beginning and <eos> at the end of the tokenized sentences \n",
    "# using a loop that iterates over the sentences in the input data\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = []\n",
    "lines = ['I am vamshi dhar', 'a data scientist']\n",
    "max_length = 0\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'I', 'am', 'vamshi', 'dhar', '<eos>'],\n",
       " ['<bos>', 'a', 'data', 'scientist', '<eos>', '<pad>']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pads the tokenized lines\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports the Dataset class and defines a list of sentences\n",
    "from torch.utils.data import Dataset\n",
    "sentences = [\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\"]\n",
    "# Downloads and reads data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "    # Returns the data length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    # Returns one item on the index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "# Creates a dataset object\n",
    "dataset=CustomDataset(sentences)\n",
    "# Accesses samples like in a list\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamshidhar/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/sampler.py:122: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /opt/conda/conda-bld/pytorch_1682343904639/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  generator.manual_seed(seed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals. Fae's a fickle friend, Harry.\"]\n"
     ]
    }
   ],
   "source": [
    "# Creates an iterator object\n",
    "# data_iter = iter(dataloader)\n",
    "# Calls the next function to return new batches of samples\n",
    "# next(data_iter)\n",
    "# Creates an instance of the custom data set\n",
    "from torch.utils.data import DataLoader\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "# Specifies a batch size\n",
    "batch_size = 2\n",
    "# Creates a data loader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Prints the sentences in each batch\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a custom collate function\n",
    "def collate_fn(batch):\n",
    "    tensor_batch = []\n",
    "# Tokenizes each sample in the batch\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "# Maps tokens to numbers using the vocab\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "# Pads the sequences within the batch to have equal lengths\n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
    "    return padded_batch\n",
    "# Creates a data loader using the collate function and the custom dataset\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
